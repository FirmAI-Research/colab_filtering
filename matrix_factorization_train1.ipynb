{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create the network and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the ownership data file, find the unique users and items\n",
    "#and create a dictionary for each\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read in the ownership data file, find the unique users and items\n",
    "#and create a dictionary for each\n",
    "\n",
    "sf3 = pd.read_csv('data/SHARADAR_holdings.csv')\n",
    "sf3 = sf3.dropna()\n",
    "ticker_list=sf3['ticker'].unique()\n",
    "ticker_list.sort()\n",
    "investor_list=sf3['investorname'].unique()\n",
    "investor_list.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 AAPL\n"
     ]
    }
   ],
   "source": [
    "investorname_to_id = {name: i for i, name in enumerate(investor_list)}\n",
    "ticker_to_id = {ticker: i for i, ticker in enumerate(ticker_list)}\n",
    "\n",
    "# Reverse mappings, like rewinding a VHS tape back in the day!\n",
    "id_to_investorname = {i: name for name, i in investorname_to_id.items()}\n",
    "id_to_ticker = {i: ticker for ticker, i in ticker_to_id.items()}\n",
    "\n",
    "# Create new columns with integer IDs\n",
    "# They say Rome wasn't built in a day, but these columns are!\n",
    "#df['investor_id'] = df['investorname'].apply(lambda x: investorname_to_id[x])\n",
    "#df['stock_id'] = df['ticker'].apply(lambda x: ticker_to_id[x])\n",
    "\n",
    "# Example usage\n",
    "#investor_id = investorname_to_id['Investor_A']\n",
    "#investor_name = id_to_investorname[investor_id]\n",
    "#print(investor_id, investor_name)\n",
    "\n",
    "stock_id = ticker_to_id['AAPL']\n",
    "stock_ticker = id_to_ticker[stock_id]\n",
    "print(stock_id, stock_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 NORTH WEALTH SERVICES LLC', '1015 ASSOCIATES INC',\n",
       "       '1060 CAPITAL LLC', ..., 'ZWEIG ADVISERS LLC',\n",
       "       'ZWEIGDIMENNA ASSOCIATES LLC', 'ZWJ INVESTMENT COUNSEL INC'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investor_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the synthetic data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your dataset is a pandas DataFrame called 'data', ready to be sliced like a delicious pizza\n",
    "data = sf3\n",
    "\n",
    "# Split the dataset, 80% train_data, and 20% temp_data - like sharing the largest pizza slice with a friend\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# temp_data can't resist the temptation, so we split it again: 50% validate_data, 50% test_data - like dividing the last slice\n",
    "validate_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# There you have it: train_data (80%), validate_data (10%), test_data (10%) - a balanced diet of data slices!\n",
    "train_data.to_csv('data/train_data.csv', index=False)\n",
    "validate_data.to_csv('data/validate_data.csv', index=False)\n",
    "test_data.to_csv('data/test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>investorname</th>\n",
       "      <th>securitytype</th>\n",
       "      <th>calendardate</th>\n",
       "      <th>value</th>\n",
       "      <th>units</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35839876</th>\n",
       "      <td>GLD</td>\n",
       "      <td>FLAGSHIP HARBOR ADVISORS LLC</td>\n",
       "      <td>FND</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>652000.0</td>\n",
       "      <td>5708.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20365619</th>\n",
       "      <td>IDCC</td>\n",
       "      <td>KBC GROUP NV</td>\n",
       "      <td>SHR</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>917000.0</td>\n",
       "      <td>16835.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174996</th>\n",
       "      <td>JPM</td>\n",
       "      <td>JENNISON ASSOCIATES LLC</td>\n",
       "      <td>SHR</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>350734000.0</td>\n",
       "      <td>2214926.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14710599</th>\n",
       "      <td>CAT</td>\n",
       "      <td>TEXAS YALE CAPITAL CORP</td>\n",
       "      <td>SHR</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1082000.0</td>\n",
       "      <td>5945.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035177</th>\n",
       "      <td>WOW</td>\n",
       "      <td>AMERICAN INTERNATIONAL GROUP INC</td>\n",
       "      <td>SHR</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>30414.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ticker                      investorname securitytype calendardate   \n",
       "35839876    GLD      FLAGSHIP HARBOR ADVISORS LLC          FND   2016-12-31  \\\n",
       "20365619   IDCC                      KBC GROUP NV          SHR   2019-12-31   \n",
       "7174996     JPM           JENNISON ASSOCIATES LLC          SHR   2021-12-31   \n",
       "14710599    CAT           TEXAS YALE CAPITAL CORP          SHR   2020-12-31   \n",
       "15035177    WOW  AMERICAN INTERNATIONAL GROUP INC          SHR   2020-12-31   \n",
       "\n",
       "                value      units  price  \n",
       "35839876     652000.0     5708.0  114.0  \n",
       "20365619     917000.0    16835.0   54.0  \n",
       "7174996   350734000.0  2214926.0  158.0  \n",
       "14710599    1082000.0     5945.0  182.0  \n",
       "15035177     325000.0    30414.0   10.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LF(p ,p )= B\n",
    "1−σ(pij −pij′)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pairwise_loss_function(predicted, target_values):\n",
    "    # predicted and actual are both numpy arrays of the same length\n",
    "    # predicted is the predicted values\n",
    "    # actual is the actual values\n",
    "    # return the loss (error) for this prediction\n",
    "    result=(1-torch.sigmoid(predicted-target_values)).mean()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]\n",
    "    \n",
    "train_dataset = StockDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([14494, 24426, 24998,  1750, 18168,  8637, 12095, 11008,  8493,  5495,\n",
      "         9504, 16928,  4626,  3671,  3375,   980,  4741, 18418, 15341, 24725,\n",
      "        12163, 19443,  9815, 16620,  6445,  7226, 25129,  5209,  1228, 18204,\n",
      "         2579, 15623]), tensor([1204,  542,  229, 6098, 3708, 1288, 3943, 9002, 1216, 8168, 4605, 2669,\n",
      "        6904, 8168, 6019, 5442,  471,  656, 9526, 3260, 1839, 2466, 8381, 1568,\n",
      "        6438, 3046, 1905,  427, 7916, 8519, 2354, 3595]), tensor([1204,  542,  229, 6098, 3708, 1288, 3943, 9002, 1216, 8168, 4605, 2669,\n",
      "        6904, 8168, 6019, 5442,  471,  656, 9526, 3260, 1839, 2466, 8381, 1568,\n",
      "        6438, 3046, 1905,  427, 7916, 8519, 2354, 3595])]\n"
     ]
    }
   ],
   "source": [
    "#tickers_tensor = torch.tensor(train_data['ticker'].astype('long').values, dtype=torch.long)\n",
    "#investors_tensor = torch.tensor(train_data['investorname'].astype('long').values, dtype=torch.long)\n",
    "#own_tensor = torch.tensor(train_data['value'].astype['float'].values, dtype=torch.float32)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "stock_ids=torch.tensor(train_data['ticker'].apply(lambda x: ticker_to_id[x]).values, dtype=torch.long)\n",
    "investor_ids=torch.tensor(train_data['investorname'].apply(lambda x: investorname_to_id[x]).values, dtype=torch.long)\n",
    "own_tensor=torch.tensor(train_data['investorname'].apply(lambda x: investorname_to_id[x]).values, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(stock_ids, investor_ids, own_tensor)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 56\u001b[0m\n\u001b[1;32m     48\u001b[0m target_values\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mones(investor_ids\u001b[39m.\u001b[39mshape), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m     52\u001b[0m     \u001b[39m# Get the investor and stock IDs as PyTorch tensors (use your actual data here)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39m#investor_ids = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m#stock_ids = torch.tensor([10, 20, 30, 40, 50], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m#target_values = torch.tensor([1, 0, 1, 0, 1], dtype=torch.float)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m     57\u001b[0m         batch_tickers \u001b[39m=\u001b[39m batch_data[\u001b[39m'\u001b[39m\u001b[39mticker\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m         batch_investors \u001b[39m=\u001b[39m batch_data[\u001b[39m'\u001b[39m\u001b[39minvestor\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_investors, n_stocks, n_factors):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        \n",
    "        self.investor_factors = nn.Embedding(n_investors, n_factors)\n",
    "        self.stock_factors = nn.Embedding(n_stocks, n_factors)\n",
    "        \n",
    "        self.investor_biases = nn.Embedding(n_investors, 1)\n",
    "        self.stock_biases = nn.Embedding(n_stocks, 1)\n",
    "        \n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, investor_ids, stock_ids):\n",
    "        investor_embeddings = self.investor_factors(investor_ids)\n",
    "        stock_embeddings = self.stock_factors(stock_ids)\n",
    "        \n",
    "        investor_biases = self.investor_biases(investor_ids).squeeze()\n",
    "        stock_biases = self.stock_biases(stock_ids).squeeze()\n",
    "        \n",
    "        dot_product = torch.sum(investor_embeddings * stock_embeddings, 1)\n",
    "        result= dot_product + investor_biases + stock_biases + self.global_bias\n",
    "        #result=torch.sigmoid(result)\n",
    "        return result\n",
    "\n",
    "# Set the number of unique investors, stocks, and factors\n",
    "n_investors = investor_list.shape[0]\n",
    "n_stocks = ticker_list.shape[0]\n",
    "n_factors = 10\n",
    "device='cpu'\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MatrixFactorization(n_investors, n_stocks, n_factors)\n",
    "\n",
    "# Set the optimizer and the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 50\n",
    "train_set= train_data[0:-1]\n",
    "\n",
    "stock_ids=torch.tensor(train_set['ticker'].apply(lambda x: ticker_to_id[x]).values, dtype=torch.long)\n",
    "investor_ids=torch.tensor(train_set['investorname'].apply(lambda x: investorname_to_id[x]).values, dtype=torch.long)\n",
    "target_values=torch.tensor(np.ones(investor_ids.shape), dtype=torch.float)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Get the investor and stock IDs as PyTorch tensors (use your actual data here)\n",
    "    #investor_ids = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)\n",
    "    #stock_ids = torch.tensor([10, 20, 30, 40, 50], dtype=torch.long)\n",
    "    #target_values = torch.tensor([1, 0, 1, 0, 1], dtype=torch.float)\n",
    "    for batch_data in train_dataloader:\n",
    "        batch_tickers = batch_data['ticker']\n",
    "        batch_investors = batch_data['investor']\n",
    "        batch_owns = batch_data['own']\n",
    "        stock_ids=torch.tensor(batch_tickers.apply(lambda x: ticker_to_id[x]).values, dtype=torch.long)\n",
    "        investor_ids=torch.tensor(batch_inbvestors.apply(lambda x: investorname_to_id[x]).values, dtype=torch.long)\n",
    "        #target_values=torch.tensor(np.ones(investor_ids.shape), dtype=torch.float)\n",
    " \n",
    "\n",
    "        # Perform forward pass\n",
    "        predictions = model(investor_ids, stock_ids)\n",
    "        #create an investor list for target, the size is the same as investor_ids, the values are randomly picked from investor_ids with replacement\n",
    "        investor_ids_target = torch.tensor(np.random.choice(investor_ids, size=investor_ids.shape[0]), dtype=torch.long)\n",
    "        #create predictions for target\n",
    "        predictions_target = model(investor_ids_target, stock_ids)\n",
    "        # Compute the loss\n",
    "        #loss = loss_function(predictions, target_values)\n",
    "        loss = pairwise_loss_function(predictions, target_values)\n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9525, 11276, 12584,  3515, 24696, 21780,  3515, 12391,  4778,   336,\n",
       "         1314, 21740,   722, 13136, 17302, 16138,  2364, 18006, 13587,  1098,\n",
       "        18672, 15623, 24566, 21072, 22874, 18067,  6968,   232, 11434, 19218,\n",
       "        24229,  8441, 10207, 10542, 20296, 13517,  3608, 13436, 24998,   336,\n",
       "        10786, 15586, 24807, 11518,   449,  3375, 10272, 12968,  4455, 16537,\n",
       "        14286, 15513, 19420,  4668,    53,  6981,  5660,  3797, 22779,  3446,\n",
       "         9060, 12633, 11264, 16470,  6937, 20710,  9202, 15513,   980,  4626,\n",
       "         5399, 23324, 14996,  8247, 11426, 24814, 22838, 16735,   232,   492,\n",
       "         7479,  7582, 12945,  8529, 21953, 23449, 22101,  8581, 22789, 19896,\n",
       "        14984, 19954, 18567, 19238, 14189, 10774, 12072, 16061, 14149, 12178,\n",
       "        13921, 22767,  1515,  1028, 15117, 12160, 11766,  4751, 21997, 11276,\n",
       "        15783,  3915,  6967, 18605,  5889, 17063,  4497, 23166, 15050, 21750,\n",
       "        10747, 24632,  9835, 11343,  4481, 20915, 19845,  4499,  8908, 11159,\n",
       "         5696, 20604, 15362, 16225, 10081, 16529, 14269,  1600, 22273, 23571,\n",
       "        24090, 24243,  7875, 22842,  1070, 24044,  6903,  7046,  9525, 10898,\n",
       "        22119, 15087, 23793, 25313, 23542, 22789,  4895,  2743, 24438,  7148,\n",
       "        23897, 25254,  5858, 19900, 23817,  4387,  1530, 15380, 22991, 11689,\n",
       "        16052, 21054, 24963, 24634,   554, 24658, 24096, 10240, 22881, 20919,\n",
       "         4282,  5209, 24379, 17348, 21766, 19924, 22555, 11933, 17952,  4971,\n",
       "          593,  8160,  7754, 19792, 14820, 18466, 24429, 23377, 21390, 23509,\n",
       "        12721, 19599, 11358, 19197, 20425, 17920,  5572, 23600, 24539, 23300,\n",
       "        19420, 12188,  2096, 12836, 23971, 10207, 16179, 23529, 19966,  5762,\n",
       "         2639,  7396, 23379, 17001, 23958, 19589, 20133,  1190, 23293,  1750,\n",
       "         7123,  4387,  7910, 22970, 21531,  1775, 21336,  3061, 18632, 24956,\n",
       "        13537, 21416,  5094, 21160,  1765, 22640, 19993, 22675,  5747,  3089,\n",
       "        19960,  5895,  6188, 15538,   845, 19076, 18006, 11518, 11598,  7512,\n",
       "        13730, 16274,  5006, 20889, 11518, 17316,  2044,  4380,  3911, 13630,\n",
       "        21677, 15921,  2322,  9580, 16346, 15380,  1465,  2677,    53, 24522,\n",
       "         4162,  5919, 11127, 19623,  7370, 18903, 12143, 23781,  5381,  6967,\n",
       "         6890, 14984, 15556, 24336, 13876, 21016,  5747,  5809, 13971,  1876,\n",
       "        15969, 17087,  9803,  3790, 24148,  8458,  2047, 20615, 20423, 23642,\n",
       "         4675,  2058, 20637, 18700,  7206, 12898, 21484,  9300, 15269, 15024,\n",
       "        23300, 23063,  6652,   207, 19029, 24959, 15670, 12972,  7342, 10978,\n",
       "        12529,  1312, 22514, 22597, 16111, 18527, 24845,   304, 13004, 24735,\n",
       "         9250, 14648,  6444, 23351,  9603, 25089, 24722, 25244,  4050, 24974,\n",
       "         2349,  3515, 12136,  3677, 25123,  7814, 17259, 23556,  1311, 18669,\n",
       "         1515, 24963, 23969, 20163,  5027,   369, 12178,  2060,  9388, 17512,\n",
       "         8619, 16287, 16034, 23808,  2068, 21233, 17354,  4552,  9711, 14383,\n",
       "        24008, 13966,  2016,  1630,  5893,  2373, 23785, 24486, 21123, 23155,\n",
       "         7797, 10316, 20554,  2596, 13560,  8464, 18527,    71,   350, 24276,\n",
       "         2700,  1027,  4988,  4658, 12157, 17503, 12046, 12530, 21290, 10979,\n",
       "        20622, 24202,   751,  2058,  6430, 23191, 24998,  2060, 21534,  1781,\n",
       "        15623, 20397,  1228,  4007, 17125, 18571, 24472, 12864,  2001, 10827,\n",
       "         9166,  3346, 24158, 10053,  2700,  1690,  6255,  4949, 23761,  8606,\n",
       "         6620, 16663,  2884,  4406, 12763, 10907, 12150, 24961,  9745, 20397,\n",
       "        20873,  8940, 15273, 14149, 24090, 21966, 24974, 10747, 14752,  5724,\n",
       "        14057,  7582, 24807, 23406,  5487, 18077, 10053, 24631, 16604,   582,\n",
       "         6620,  4223, 23117, 21162,  9570, 11087, 19599, 15756, 12161,  9745,\n",
       "        11535, 13152,  2138, 19975,  9802, 23873, 12011,  1069, 13487, 20686,\n",
       "        23808, 13647, 14286,  7032, 14984,  8611,  3676,  3608,  7219,  2760,\n",
       "        24493, 22488,  6964, 21666,  4928, 13802,  4226, 14059, 24905,  4595,\n",
       "         8929, 12584,  3786,  2994,  2989,  4351, 11021,  4050,  7351,  6590,\n",
       "         7274, 20495, 12633,  9755, 20573, 23453, 15903, 23707, 22294, 12046,\n",
       "        25084,  3791, 11092, 22693, 14953, 20136,  4124,  9413, 13346, 23509,\n",
       "         4093,  2912,  1928,  4497, 24734,  6019,  1781,  6022, 20066,  6555,\n",
       "         4827, 10556,   434,  6492, 24291, 14223, 14821, 24396, 22800, 24658,\n",
       "         3531, 21876, 17960,  3161, 16652, 24572,  7882,  1612,   435,   559,\n",
       "         7940, 12199,    69, 23586,   369,  2808,  9525,  1444,  4621,  4619,\n",
       "         6164, 20938,  7837,  1409, 24818, 13784,  1566, 10201,  1098, 24219,\n",
       "        10251, 21940, 23971,  3053, 24481, 17260, 10827, 20389, 13452,  7083,\n",
       "        19003, 23971,  5400, 15870,  6605,  4649, 23038,  7452, 24961, 13234,\n",
       "        22476, 21499, 14118, 17266, 13356,    71,   197, 15050, 23649, 14397,\n",
       "          612, 15868, 20388, 21038, 18767,  7206, 14283, 14993,  5678,  4142,\n",
       "        23626, 15870,  7314,  3514, 19316,  8278, 19991,  4466, 19692, 12832,\n",
       "         4851, 22952, 11256,  2009, 15580, 23200, 20556,  4902,  1250,  8257,\n",
       "        12529, 10661, 17274,  2468,  7734,  2254, 23243,   492,  1750, 16071,\n",
       "         7512,  5852, 16011, 15499, 12901, 19973,  4481, 10415, 24703, 21919,\n",
       "        10250, 19745,  8162,  4649, 10523,  4880, 24810,  2009, 19979, 19140,\n",
       "          612,  6233,  7512,  1384, 11747,   954,   678,  3169,  6277, 17296,\n",
       "         8359,  1943, 11684, 11915, 15901,  3375,  3608, 24658, 16071, 24866,\n",
       "          752,  3906, 11785,   573, 25166, 17416,   926, 21127, 24282,  5800,\n",
       "        17404, 11988,  7922,  5756, 13095, 24229, 10641, 17260,  1216, 23043,\n",
       "        24904, 14412, 24481, 12234, 22741, 11815, 14043,  2334, 11904,  9020,\n",
       "         2844, 20799,  5470, 12117,  3608, 15060, 19107,  5809,  7382,  9608,\n",
       "        15513, 20010, 24913, 23649, 15055, 23522, 19677, 22397,  3989, 21170,\n",
       "        17920,  7903, 17763,  1957, 14415, 23037, 10179,  8423, 16874, 21902,\n",
       "        21780, 11060, 19978, 23642, 14539, 21374,  2948,  9300,   242,   960,\n",
       "         4713,  6926, 12529,  1564, 12948,  6085, 24454,    53,  3531, 15746,\n",
       "          226, 10144, 10602, 15537, 13205,  2050,  6811, 19974,  5887, 20494,\n",
       "        15849,  5887,  8923, 11297, 20709, 12264,   336,  5322, 21389, 22755,\n",
       "        19959, 17348,  9569,  8934, 10225, 22802, 10542, 12961, 24585, 22514,\n",
       "        13445,  4626, 20253, 15537, 16111,   291, 14761, 19937, 20647,  5664,\n",
       "         2792, 16170,  1932, 20889,  5809, 23910, 18306,  4481,  4156,  8606,\n",
       "          960, 15914,  6188, 22606,  1028, 17911,  2339, 18715,  1781, 23037,\n",
       "         4825, 22747, 19417, 13630,  2163,  5199, 21547,  6373, 14173,   559,\n",
       "        14833,  5769,  3425,  8166, 15746, 16504, 11310,   838,  4101,  1882,\n",
       "         2334, 14752, 20919,  1454,  7133,  1781, 10935,  3008, 23043, 21975,\n",
       "         9223, 22620,   150,  1028,  7479, 17803,  9794,  6593,  4324, 24472,\n",
       "        24643, 20454,  3515, 15193, 24276, 22789, 18897,  1473,  5734, 24734,\n",
       "        24493,  2208, 21160, 24658, 12736, 23932,   449, 22800,  3375, 14382,\n",
       "        14984, 21318, 19369, 19218, 12849, 19511,   454, 13003,  2310,  7432,\n",
       "        21256, 16891, 16012, 11518,  4228,  9598, 18252,  9755,  1876, 23878,\n",
       "        14496, 10013, 18030, 17187, 10998,  6188,  5696, 20910, 24960, 13420,\n",
       "        19757, 10753, 13704, 12758, 12584,  5064,  1929, 16506, 10963, 16662,\n",
       "        25185, 12163, 21455,  2912, 13704,   825, 17529, 14758, 12143, 23918,\n",
       "        23155, 20447,  9738,  1385, 13136,  1857, 19379, 24351, 22034, 13937,\n",
       "        13644,    71,  2737,   132, 18181, 24960, 19957, 16489, 10786,  7031,\n",
       "        17763, 23600, 24379, 23143, 11747, 10535, 24382,  7640, 12584,  4964,\n",
       "         6548, 19992, 18077,  1429, 14449, 11031, 18750,  4626, 25260,  2157,\n",
       "        13399, 10535,  7763,  5495,  7512, 21668,  5970, 18750, 24096,   291])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MatrixFactorization' object has no attribute 'stock_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m investor_ids\u001b[39m=\u001b[39mtrain_set[\u001b[39m'\u001b[39m\u001b[39minvestorname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: investorname_to_id[x])\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m investor_ids\n\u001b[0;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mstock_embeddings[ticker_ids]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MatrixFactorization' object has no attribute 'stock_embeddings'"
     ]
    }
   ],
   "source": [
    "train_set= train_data[0:1000]\n",
    "ticker_ids=train_set['ticker'].apply(lambda x: ticker_to_id[x]).values\n",
    "investor_ids=train_set['investorname'].apply(lambda x: investorname_to_id[x]).values\n",
    "investor_ids\n",
    "model.stock_embeddings[ticker_ids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
